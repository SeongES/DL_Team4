# DL_Team4

## Project Title  
**Assistance in Car Roads for Visual-Impaired People**  
> A deep learning system that detects road objects, estimates their distance, and provides real-time audio alerts to assist visually-impaired individuals in navigating car roads.

---

## ğŸ”§ Setup

### 1. Dataset Download

We use the **BDD100K dataset** for object detection.

- ğŸ“¦ [Download BDD100K Dataset from Kaggle](https://www.kaggle.com/datasets/solesensei/solesensei_bdd100k)

### 2. Pretrained Models and Annotations

Please download the following files and place them in the appropriate directories as specified in the notebooks:

- ğŸ“„ [`gt_depths.npz`](https://drive.google.com/file/d/1OpMHwBZrsHzO-lOheG2lbpUGAmow-L_-/view?usp=sharing) â€” Precomputed ground truth depths
- ğŸ“„ [`bdd100k_labels_images_train_coco.json`](https://drive.google.com/file/d/1cqueQXoroEzOQc874oo6jjb5nTvermVx/view?usp=sharing) â€” BDD100K annotations in COCO format
- ğŸ“„ [`best.pt`](https://drive.google.com/file/d/1Yqk-uYAM65qdumR4kOspXOrYExv4Ex1W/view?usp=sharing) - 'best.pt' file of finetuned YOLO
- ğŸ“„ [`models`](https://surfdrive.surf.nl/files/index.php/s/CUjiK221EFLyXDY) - Depth-estimation weight from Lite-Mono.

---

## ğŸ—‚ï¸ File Structure 
- Download models/ and gt_depths.npz, bdd100k_labels_images_train_coco.json as the following structure.
```
project/
â”œâ”€â”€ models
â”œâ”€â”€â”€â”€lite-mono_640x192
â”œâ”€â”€â”€â”€â”€â”€ depth.pth
â”œâ”€â”€â”€â”€â”€â”€ encoder.pth
â”œâ”€â”€â”€â”€best.pt
â”œâ”€â”€ bdd100k_labels_images_train_coco.json
â”œâ”€â”€ gt_depths.npz
â””â”€â”€ demo.py
```
---

## ğŸ“ File Descriptions

### ğŸ”¹ `train.ipynb`  
- YOLOv11n object detection model training notebook  
- Finetunes YOLOv11n on the BDD100K dataset

### ğŸ”¹ `depth_estimation.ipynb`  
- Performs monocular depth estimation using **LiteMono**  
- Based on the official [LiteMono GitHub repository](https://github.com/noahzn/Lite-Mono)

### ğŸ”¹ `demo.py`  
- Real-time system integrating:
  - Object Detection (YOLO)
  - Depth Estimation (LiteMono)
  - Text-to-Speech (TTS) alerts  
- Designed to assist visually-impaired users by announcing detected objects and their relative distance


## To Run Demo File

#### Run and save all files from a directory
    python demo.py --mode all --input <input_directory> --output <output_directory>
    ex. python demo.py --mode all --input ../test --output ../out

#### Just run single file
    python demo.py --mode single_infer --input <input_file>
    ex. python script.py --mode single --input ../test/sample.mp4



